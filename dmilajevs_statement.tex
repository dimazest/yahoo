\documentclass[11pt]{article}

\usepackage{setspace}
\usepackage{fullpage}
\usepackage{parskip}

\usepackage[T1]{fontenc}
\usepackage{type1cm}
\usepackage{lmodern}
\renewcommand*\familydefault{\sfdefault}

\title{Research statement}
\author{
Dmitrijs Milajevs \\
% {\tt d.milajevs@qmul.ac.uk} \\
}
\date{}

\begin{document}

\maketitle

\thispagestyle{empty}

Language has always intrigued me, and the fact that it is possible to capture such a complex phenomenon in computational constructs was a massive eye-opener during my undergraduate studies. My fascination for this has brought me where I am now, in the final stage of my PhD in computational linguistics at Queen Mary University of London.

My thesis is about distributional semantics, a field that learns word meaning from naturally occurring text and stores it in algebraic constructs, namely vectors. My work is driven by the aim to extend the "word-level" methods to sentences and documents. This will enable the application of distributional semantics to dialogue processing, paraphrase detection, document clustering and classification, and many other NLP challenges.

The methods I follow build the representation of sentence meaning the way a chef prepares a dish: word vectors are combined according to an algebraic recipe, resulting in a single representation for the utterance as a whole. The recipe of algebraic operations is defined by the grammatical structure of the sentence. Likewise, my work combines elements of statistical NLP, linear algebra and formal semantics.

Currently I am investigating the use of this technique for information retrieval. Traditional IR systems operate on bag-of-words models, making it difficult to capture relationships between words, and require extensions such as query expansion. Models based on distributional semantics can handle this naturally without further extension. It would make a fabulous addition if I could try out my techniques in an IR system, and on actual large data, as is available in Yahoo Labs.

The resulting models are complex, as there are so many parameters involved, which I systematically explore in my studies. The experimental pipeline I have developed lets me evaluate thousands of models on several datasets over night. This experience will allow me quickly get started with the experiments at Yahoo.

Yahoo's activities caught my interest after listening to Daniele Quercia's talk at WWW 2013, which was on the topic of how the attractiveness of London boroughs affects the well-being of their residents. The talk demonstrated the breath of the research carried out at Yahoo Labs.

I also admire the way Yahoo applications stand out, for instance, Yahoo News, the app which generates news stories algorithmically and shows a comprehensive summary of the current events around the world.

I would love to get the opportunity to work with Fabrizio Silvestri and Peter Mika to see how distributional word and sentence meaning representations can be applied to retrieval and advertisement, and I look forward to your reply.

\end{document}
